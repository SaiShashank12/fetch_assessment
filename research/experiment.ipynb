{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLGhobbYzNUI",
        "outputId": "17208328-d99d-4947-a4c6-04dad99a85cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "365\n",
            "(284, 7, 8) (284,) (71, 7, 8) (71,)\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 3s 54ms/step - loss: 0.2052 - val_loss: 0.1238\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0613 - val_loss: 0.0202\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0287 - val_loss: 0.0116\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0159 - val_loss: 0.0125\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0137 - val_loss: 0.0058\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0112 - val_loss: 0.0042\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0108 - val_loss: 0.0036\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0108 - val_loss: 0.0038\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0104 - val_loss: 0.0036\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 0.0041\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0086 - val_loss: 0.0042\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0094 - val_loss: 0.0032\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0092 - val_loss: 0.0048\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0093 - val_loss: 0.0033\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0095 - val_loss: 0.0034\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0073 - val_loss: 0.0034\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0075 - val_loss: 0.0048\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0077 - val_loss: 0.0034\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 68ms/step - loss: 0.0081 - val_loss: 0.0055\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0100 - val_loss: 0.0038\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0084 - val_loss: 0.0048\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0076 - val_loss: 0.0037\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CBDEA37280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "(71, 1) (71,)\n",
            "Test RMSE: 0.0707650859955579\n",
            "Test RMSE on original scale: 250804.06095356448\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout,SimpleRNN,Conv1D,MaxPooling1D,Flatten,Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load and process the dataset\n",
        "data = pd.read_csv(r'C:\\Users\\shash\\fetch_assessment\\artifacts\\data_ingestion\\data_daily.csv')\n",
        "\n",
        "# Adding additional features\n",
        "data['# Date'] = pd.to_datetime(data['# Date'])\n",
        "data['Day_of_Week'] = data['# Date'].dt.dayofweek\n",
        "data['Month'] = data['# Date'].dt.month\n",
        "data['Day'] = data['# Date'].dt.day\n",
        "data['Year'] = data['# Date'].dt.year\n",
        "data['Lag_1'] = data['Receipt_Count'].shift(1)\n",
        "data['Lag_2'] = data['Receipt_Count'].shift(2)\n",
        "data['Lag_3'] = data['Receipt_Count'].shift(3)\n",
        "print(len(data))\n",
        "# Dropping rows with NaN values after adding lag features\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data.drop(['# Date'], axis=1))\n",
        "\n",
        "# Function to create dataset\n",
        "def create_dataset(data, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:(i + look_back), :])\n",
        "        y.append(data[i + look_back, 0])  # 0 index for 'Receipt_Count'\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Creating the dataset with look back\n",
        "look_back = 7\n",
        "X, y = create_dataset(scaled_data, look_back)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, y_train.shape,X_test.shape,y_test.shape)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "# CNN Layer\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, X.shape[2])))\n",
        "# Removing MaxPooling1D layer to maintain the sequence length\n",
        "# LSTM Layer\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "# Output Layer\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "# Predicting on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred.shape,y_test.shape)\n",
        "# Calculating RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Test RMSE:\", rmse)\n",
        "# Reshape y_pred_scaled for inverse transformation\n",
        "temp_shape = np.zeros((len(y_pred), scaled_data.shape[1]))\n",
        "temp_shape[:, 0] = y_pred[:, 0]\n",
        "y_pred = scaler.inverse_transform(temp_shape)[:, 0]\n",
        "\n",
        "# Reshape y_test for inverse transformation\n",
        "y_test_temp_shape = np.zeros((len(y_test), scaled_data.shape[1]))\n",
        "y_test_temp_shape[:, 0] = y_test\n",
        "y_test_rescaled = scaler.inverse_transform(y_test_temp_shape)[:, 0]\n",
        "\n",
        "# Calculating RMSE on the rescaled data\n",
        "rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred))\n",
        "print(\"Test RMSE on original scale:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQsgGBb1IVQw"
      },
      "source": [
        "## LSTM CNN:0.07481992549716468\n",
        "## LSTM: 0.08222140069384448\n",
        "## RNN: 0.08422675959263284\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CQD1cQ8xE7EC"
      },
      "outputs": [],
      "source": [
        "# def predict_next_n_days(model, recent_data, n_days, scaler):\n",
        "#     \"\"\"\n",
        "#     Predict the next 'n_days' days using the trained 'model' and 'recent_data'.\n",
        "\n",
        "#     :param model: The trained forecasting model.\n",
        "#     :param recent_data: A numpy array of the most recent data points (including features).\n",
        "#     :param n_days: Number of days to predict into the future.\n",
        "#     :param scaler: The MinMaxScaler used to scale the data.\n",
        "#     :return: A list of predicted values for the next 'n_days' days.\n",
        "#     \"\"\"\n",
        "#     future_predictions = []\n",
        "#     current_batch = recent_data.copy()\n",
        "\n",
        "#     for i in range(n_days):\n",
        "#         # Predict the next time step\n",
        "#         current_pred = model.predict(current_batch[np.newaxis, :])[0, 0]\n",
        "        \n",
        "#         # Append the prediction to the list\n",
        "#         future_predictions.append(current_pred)\n",
        "        \n",
        "#         # Update the current batch to include the new prediction and update other features\n",
        "#         current_batch = np.roll(current_batch, -1, axis=0)\n",
        "#         # Update lagged Receipt_Count\n",
        "#         current_batch[-1, 0] = current_pred\n",
        "#         # Update other features if necessary (like date components)\n",
        "\n",
        "#     # Rescale the predictions back to the original scale\n",
        "#     temp_shape = np.zeros((len(future_predictions), scaled_data.shape[1]))\n",
        "#     temp_shape[:, 0] = future_predictions\n",
        "#     future_predictions_rescaled = scaler.inverse_transform(temp_shape)[:, 0]\n",
        "\n",
        "#     return future_predictions_rescaled\n",
        "\n",
        "# # Example usage\n",
        "# recent_data = X_test[-1]  # Last row of the test data\n",
        "# n_days = 10  # Number of days to predict\n",
        "# predictions = predict_next_n_days(model, recent_data, n_days, scaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9443955.81066763, 9457532.1804018 , 9455146.96262801,\n",
              "       9445791.14606917, 9444906.85578477, 9452504.65424836,\n",
              "       9453441.9681499 , 9444955.44316304, 9460873.09078109,\n",
              "       9460126.95765054])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"../\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fetch_assessment.pipeline.predict import PredictionPipleline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-11-13 22:46:01,437: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
            "[2023-11-13 22:46:01,437: INFO: common: yaml file: params.yaml loaded successfully]\n",
            "[2023-11-13 22:46:01,441: INFO: common: created directory at: artifacts]\n",
            "[2023-11-13 22:46:01,445: INFO: common: created directory at: artifacts/model_training]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([9415045.26435351, 9419317.99614811, 9384337.61879206,\n",
              "       9350605.51455843, 9331892.19144499, 9383879.62993956,\n",
              "       9406431.98968422, 9408507.51573396, 9416416.4846679 ,\n",
              "       9388182.35915899])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d=PredictionPipleline()\n",
        "d.predict(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
