{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLGhobbYzNUI",
        "outputId": "17208328-d99d-4947-a4c6-04dad99a85cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 3s 47ms/step - loss: 0.1797 - val_loss: 0.0506\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0313 - val_loss: 0.0215\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0176 - val_loss: 0.0181\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0174 - val_loss: 0.0076\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.0124 - val_loss: 0.0059\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0099 - val_loss: 0.0056\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0095 - val_loss: 0.0045\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 0.0035\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0092 - val_loss: 0.0034\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0081 - val_loss: 0.0044\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0084 - val_loss: 0.0032\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0090 - val_loss: 0.0031\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.0100 - val_loss: 0.0041\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0045\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0085 - val_loss: 0.0038\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0078 - val_loss: 0.0049\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0039\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.0077 - val_loss: 0.0046\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "Test RMSE: 0.07079313403373098\n",
            "Test RMSE on original scale: 250903.46819340062\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout,SimpleRNN,Conv1D,MaxPooling1D,Flatten,Reshape\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load and process the dataset\n",
        "data = pd.read_csv(r'C:\\Users\\shash\\fetch_assessment\\artifacts\\data_ingestion\\data_daily.csv')\n",
        "\n",
        "# Adding additional features\n",
        "data['# Date'] = pd.to_datetime(data['# Date'])\n",
        "data['Day_of_Week'] = data['# Date'].dt.dayofweek\n",
        "data['Month'] = data['# Date'].dt.month\n",
        "data['Day'] = data['# Date'].dt.day\n",
        "data['Year'] = data['# Date'].dt.year\n",
        "data['Lag_1'] = data['Receipt_Count'].shift(1)\n",
        "data['Lag_2'] = data['Receipt_Count'].shift(2)\n",
        "data['Lag_3'] = data['Receipt_Count'].shift(3)\n",
        "\n",
        "# Dropping rows with NaN values after adding lag features\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data.drop(['# Date'], axis=1))\n",
        "\n",
        "# Function to create dataset\n",
        "def create_dataset(data, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:(i + look_back), :])\n",
        "        y.append(data[i + look_back, 0])  # 0 index for 'Receipt_Count'\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Creating the dataset with look back\n",
        "look_back = 7\n",
        "X, y = create_dataset(scaled_data, look_back)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, y_train.shape,X_test.shape,y_test.shape)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "# CNN Layer\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, X.shape[2])))\n",
        "# Removing MaxPooling1D layer to maintain the sequence length\n",
        "# LSTM Layer\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "# Output Layer\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "# Predicting on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Test RMSE:\", rmse)\n",
        "# Reshape y_pred_scaled for inverse transformation\n",
        "temp_shape = np.zeros((len(y_pred), scaled_data.shape[1]))\n",
        "temp_shape[:, 0] = y_pred[:, 0]\n",
        "y_pred = scaler.inverse_transform(temp_shape)[:, 0]\n",
        "\n",
        "# Reshape y_test for inverse transformation\n",
        "y_test_temp_shape = np.zeros((len(y_test), scaled_data.shape[1]))\n",
        "y_test_temp_shape[:, 0] = y_test\n",
        "y_test_rescaled = scaler.inverse_transform(y_test_temp_shape)[:, 0]\n",
        "\n",
        "# Calculating RMSE on the rescaled data\n",
        "rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred))\n",
        "print(\"Test RMSE on original scale:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQsgGBb1IVQw"
      },
      "source": [
        "## LSTM CNN:0.07481992549716468\n",
        "## LSTM: 0.08222140069384448\n",
        "## RNN: 0.08422675959263284\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQD1cQ8xE7EC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
